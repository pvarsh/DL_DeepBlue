\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times,cite}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Deep Learning Paper Template}


\author{
Priyank Bhatia \\
New York University \\
Center for Urban Science + Progress \\
1 MetroTech Center, 19th Floor \\
Brooklyn, NY 11201 \\
\texttt{pb1672@nyu.edu} \\
\AND
Emil Christensen \\
New York University \\
Center for Urban Science + Progress \\
1 MetroTech Center, 19th Floor \\
Brooklyn, NY 11201 \\
\texttt{erc399@nyu.edu} \\
\And
Peter Varshavsky \\
New York University \\
Center for Urban Science + Progress \\
1 MetroTech Center, 19th Floor \\
Brooklyn, NY 11201 \\
\texttt{pv629@nyu.edu} \\
}


% Some example code:

%\begin{center}
%   \url{URLs go here}
%\end{center}

%\section{A section!}
%\subsection{A subsection!}

%\label{sub_1} - label a section
%\ref{sub_1} - reference a labeled section

%\footnote{Sample of a footnote}

% Figures:
% \begin{figure}[h]
% \begin{center}
% \framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% Tables:
% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% Lists (can be recursive):
% \begin{itemize}
% \item Item 1
% \item Item 2
% \item Item 3
% \end{itemize}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\nipsfinalcopy % Uncomment for camera-ready version


\begin{document}

\maketitle


\begin{abstract}
The abstract paragraph goes here!
\end{abstract}

\section{Introduction}
\label{intro}
The architecture of the model provided is a convolutional neural network with two stages and a classifier. The input has 3 feature maps, each 32x32 pixels.The first stage performs convolutions using a tanh squashing function with a 5x5 filter to produce 64 feature maps to which $L^2$ pooling with pooling size 2x2 is applied. A subtractive normalization module using a Gaussian kernel of size 7 is then applied before feeding the 64 14x14 outputs to the next stage.The second stage performs the convolutions and poolings with the same filter sizes, pool sizes, number of feature maps, and normalization function as the first stage except that the 16-dim feature maps are projected into 256-dim maps before feeding the resulting data into a 2-layer non linear classifier with 128 hidden units, which uses the tanh function for non linear transformation. A Class Negative Log-Likelihood (nll) Loss function is used along with Stochastic Gradient Descent (SGD) for optimization and learning rate of 1e-3, mini-batch size of 1, and momentum 0 number of max iterations is equal to 1. Despite our attempts to improve the model, we found that the original model (as provided by Clement Farabetâ€™s tutorial) yielded the best results.
Using dataset cited: ~\cite{37648}

\section{Architecture}
\label{arc}

\section{Learning Techniques}
\label{learn}

\section{Training Procedure}
\label{train}

\section{Results}
\label{res}


%\subsubsection*{References}
\bibliography{citations}{}
\bibliographystyle{plain}
%\small{
% [1] 
% }

\end{document}